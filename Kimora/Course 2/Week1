Assignment1:
  # GRADED FUNCTION: initialize_parameters_zeros 
  
  def initialize_parameters_zeros(layers_dims):
      """
      Arguments:
      layer_dims -- python array (list) containing the size of each layer.
      
      Returns:
      parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
                      W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
                      b1 -- bias vector of shape (layers_dims[1], 1)
                      ...
                      WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
                      bL -- bias vector of shape (layers_dims[L], 1)
      """
      
      parameters = {}
      L = len(layers_dims)            # number of layers in the network
      
      for l in range(1, L):
          #(≈ 2 lines of code)
          # parameters['W' + str(l)] = 
          # parameters['b' + str(l)] = 
          # YOUR CODE STARTS HERE
          parameters['W' + str(l)] = np.zeros((layers_dims[l],layers_dims[l-1]))
          parameters['b' + str(l)] = np.zeros((layers_dims[l],1))
          
          # YOUR CODE ENDS HERE
      return parameters
  
  
  
  # GRADED FUNCTION: initialize_parameters_random
  
  def initialize_parameters_random(layers_dims):
      """
      Arguments:
      layer_dims -- python array (list) containing the size of each layer.
      
      Returns:
      parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
                      W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
                      b1 -- bias vector of shape (layers_dims[1], 1)
                      ...
                      WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
                      bL -- bias vector of shape (layers_dims[L], 1)
      """
      
      np.random.seed(3)               # This seed makes sure your "random" numbers will be the as ours
      parameters = {}
      L = len(layers_dims)            # integer representing the number of layers
      
      for l in range(1, L):
          #(≈ 2 lines of code)
          # parameters['W' + str(l)] = 
          # parameters['b' + str(l)] =
          # YOUR CODE STARTS HERE
          parameters['W' + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1]) * 10
          parameters['b' + str(l)] = np.zeros((layers_dims[l],1))
          
          # YOUR CODE ENDS HERE
  
      return parameters
  
  
  
  # GRADED FUNCTION: initialize_parameters_he
  
  def initialize_parameters_he(layers_dims):
      """
      Arguments:
      layer_dims -- python array (list) containing the size of each layer.
      
      Returns:
      parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
                      W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
                      b1 -- bias vector of shape (layers_dims[1], 1)
                      ...
                      WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
                      bL -- bias vector of shape (layers_dims[L], 1)
      """
      
      np.random.seed(3)
      parameters = {}
      L = len(layers_dims) - 1 # integer representing the number of layers
       
      for l in range(1, L + 1):
          #(≈ 2 lines of code)
          # parameters['W' + str(l)] = 
          # parameters['b' + str(l)] =
          # YOUR CODE STARTS HERE
          parameters['W' + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1]) * np.sqrt(2./layers_dims[l-1])
          parameters['b' + str(l)] = np.zeros((layers_dims[l],1))
          
          # YOUR CODE ENDS HERE
          
      return parameters



Assignment2:
  # GRADED FUNCTION: compute_cost_with_regularization
  
  def compute_cost_with_regularization(A3, Y, parameters, lambd):
      """
      Implement the cost function with L2 regularization. See formula (2) above.
      
      Arguments:
      A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)
      Y -- "true" labels vector, of shape (output size, number of examples)
      parameters -- python dictionary containing parameters of the model
      
      Returns:
      cost - value of the regularized loss function (formula (2))
      """
      m = Y.shape[1]
      W1 = parameters["W1"]
      W2 = parameters["W2"]
      W3 = parameters["W3"]
      
      cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost
      
      #(≈ 1 lines of code)
      # L2_regularization_cost = 
      # YOUR CODE STARTS HERE
      L2_regularization_cost = (1/m)*(lambd/2)*np.sum(np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)))
      
      # YOUR CODE ENDS HERE
      
      cost = cross_entropy_cost + L2_regularization_cost
      
      return cost
  
  
  
  # GRADED FUNCTION: backward_propagation_with_regularization
  
  def backward_propagation_with_regularization(X, Y, cache, lambd):
      """
      Implements the backward propagation of our baseline model to which we added an L2 regularization.
      
      Arguments:
      X -- input dataset, of shape (input size, number of examples)
      Y -- "true" labels vector, of shape (output size, number of examples)
      cache -- cache output from forward_propagation()
      lambd -- regularization hyperparameter, scalar
      
      Returns:
      gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables
      """
      
      m = X.shape[1]
      (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache
      
      dZ3 = A3 - Y
      #(≈ 1 lines of code)
      # dW3 = 1./m * np.dot(dZ3, A2.T) + None
      # YOUR CODE STARTS HERE
      dW3 = 1./m * np.dot(dZ3, A2.T) + (lambd/m)*W3
      
      # YOUR CODE ENDS HERE
      db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)
      
      dA2 = np.dot(W3.T, dZ3)
      dZ2 = np.multiply(dA2, np.int64(A2 > 0))
      #(≈ 1 lines of code)
      # dW2 = 1./m * np.dot(dZ2, A1.T) + None
      # YOUR CODE STARTS HERE
      dW2 = 1./m * np.dot(dZ2, A1.T) + (lambd/m)*W2
      
      # YOUR CODE ENDS HERE
      db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)
      
      dA1 = np.dot(W2.T, dZ2)
      dZ1 = np.multiply(dA1, np.int64(A1 > 0))
      #(≈ 1 lines of code)
      # dW1 = 1./m * np.dot(dZ1, X.T) + None
      # YOUR CODE STARTS HERE
      dW1 = 1./m * np.dot(dZ1, X.T) + (lambd/m)*W1
      
      # YOUR CODE ENDS HERE
      db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)
      
      gradients = {"dZ3": dZ3, "dW3": dW3, "db3": db3,"dA2": dA2,
                   "dZ2": dZ2, "dW2": dW2, "db2": db2, "dA1": dA1, 
                   "dZ1": dZ1, "dW1": dW1, "db1": db1}
      
      return gradients
  
  
  
  # GRADED FUNCTION: forward_propagation_with_dropout
  
  def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):
      """
      Implements the forward propagation: LINEAR -> RELU + DROPOUT -> LINEAR -> RELU + DROPOUT -> LINEAR -> SIGMOID.
      
      Arguments:
      X -- input dataset, of shape (2, number of examples)
      parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":
                      W1 -- weight matrix of shape (20, 2)
                      b1 -- bias vector of shape (20, 1)
                      W2 -- weight matrix of shape (3, 20)
                      b2 -- bias vector of shape (3, 1)
                      W3 -- weight matrix of shape (1, 3)
                      b3 -- bias vector of shape (1, 1)
      keep_prob - probability of keeping a neuron active during drop-out, scalar
      
      Returns:
      A3 -- last activation value, output of the forward propagation, of shape (1,1)
      cache -- tuple, information stored for computing the backward propagation
      """
      
      np.random.seed(1)
      
      # retrieve parameters
      W1 = parameters["W1"]
      b1 = parameters["b1"]
      W2 = parameters["W2"]
      b2 = parameters["b2"]
      W3 = parameters["W3"]
      b3 = parameters["b3"]
      
      # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID
      Z1 = np.dot(W1, X) + b1
      A1 = relu(Z1)
      #(≈ 4 lines of code)         # Steps 1-4 below correspond to the Steps 1-4 described above. 
      # D1 =                                           # Step 1: initialize matrix D1 = np.random.rand(..., ...)
      # D1 =                                           # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)
      # A1 =                                           # Step 3: shut down some neurons of A1
      # A1 =                                           # Step 4: scale the value of neurons that haven't been shut down
      # YOUR CODE STARTS HERE
      D1 = np.random.rand(A1.shape[0],A1.shape[1])
      D1 = (D1 < keep_prob).astype(int)
      A1 = A1*D1
      A1 = A1/keep_prob
      
      # YOUR CODE ENDS HERE
      Z2 = np.dot(W2, A1) + b2
      A2 = relu(Z2)
      #(≈ 4 lines of code)
      # D2 =                                           # Step 1: initialize matrix D2 = np.random.rand(..., ...)
      # D2 =                                           # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)
      # A2 =                                           # Step 3: shut down some neurons of A2
      # A2 =                                           # Step 4: scale the value of neurons that haven't been shut down
      # YOUR CODE STARTS HERE
      D2 = np.random.rand(A2.shape[0],A2.shape[1])
      D2 = (D2 < keep_prob).astype(int)
      A2 = A2*D2
      A2 = A2/keep_prob
      
      # YOUR CODE ENDS HERE
      Z3 = np.dot(W3, A2) + b3
      A3 = sigmoid(Z3)
      
      cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)
      
      return A3, cache
  
  
  
  # GRADED FUNCTION: backward_propagation_with_dropout
  
  def backward_propagation_with_dropout(X, Y, cache, keep_prob):
      """
      Implements the backward propagation of our baseline model to which we added dropout.
      
      Arguments:
      X -- input dataset, of shape (2, number of examples)
      Y -- "true" labels vector, of shape (output size, number of examples)
      cache -- cache output from forward_propagation_with_dropout()
      keep_prob - probability of keeping a neuron active during drop-out, scalar
      
      Returns:
      gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables
      """
      
      m = X.shape[1]
      (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache
      
      dZ3 = A3 - Y
      dW3 = 1./m * np.dot(dZ3, A2.T)
      db3 = 1./m * np.sum(dZ3, axis=1, keepdims=True)
      dA2 = np.dot(W3.T, dZ3)
      #(≈ 2 lines of code)
      # dA2 =                # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation
      # dA2 =                # Step 2: Scale the value of neurons that haven't been shut down
      # YOUR CODE STARTS HERE
      dA2 = dA2*D2
      dA2 = dA2/keep_prob
      
      # YOUR CODE ENDS HERE
      dZ2 = np.multiply(dA2, np.int64(A2 > 0))
      dW2 = 1./m * np.dot(dZ2, A1.T)
      db2 = 1./m * np.sum(dZ2, axis=1, keepdims=True)
      
      dA1 = np.dot(W2.T, dZ2)
      #(≈ 2 lines of code)
      # dA1 =                # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation
      # dA1 =                # Step 2: Scale the value of neurons that haven't been shut down
      # YOUR CODE STARTS HERE
      dA1 = dA1*D1
      dA1 = dA1/keep_prob
      
      # YOUR CODE ENDS HERE
      dZ1 = np.multiply(dA1, np.int64(A1 > 0))
      dW1 = 1./m * np.dot(dZ1, X.T)
      db1 = 1./m * np.sum(dZ1, axis=1, keepdims=True)
      
      gradients = {"dZ3": dZ3, "dW3": dW3, "db3": db3,"dA2": dA2,
                   "dZ2": dZ2, "dW2": dW2, "db2": db2, "dA1": dA1, 
                   "dZ1": dZ1, "dW1": dW1, "db1": db1}
      
      return gradients



Assignment3:
# GRADED FUNCTION: forward_propagation

def forward_propagation(x, theta):
    """
    Implement the linear forward propagation (compute J) presented in Figure 1 (J(theta) = theta * x)
    
    Arguments:
    x -- a real-valued input
    theta -- our parameter, a real number as well
    
    Returns:
    J -- the value of function J, computed using the formula J(theta) = theta * x
    """
    
    # (approx. 1 line)
    # J = 
    # YOUR CODE STARTS HERE
    J = theta * x
    
    # YOUR CODE ENDS HERE
    
    return J



# GRADED FUNCTION: backward_propagation

def backward_propagation(x, theta):
    """
    Computes the derivative of J with respect to theta (see Figure 1).
    
    Arguments:
    x -- a real-valued input
    theta -- our parameter, a real number as well
    
    Returns:
    dtheta -- the gradient of the cost with respect to theta
    """
    
    # (approx. 1 line)
    # dtheta = 
    # YOUR CODE STARTS HERE
    dtheta = x
    
    # YOUR CODE ENDS HERE
    
    return dtheta



# GRADED FUNCTION: gradient_check

def gradient_check(x, theta, epsilon=1e-7, print_msg=False):
    """
    Implement the gradient checking presented in Figure 1.
    
    Arguments:
    x -- a float input
    theta -- our parameter, a float as well
    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)
    
    Returns:
    difference -- difference (2) between the approximated gradient and the backward propagation gradient. Float output
    """
    
    # Compute gradapprox using right side of formula (1). epsilon is small enough, you don't need to worry about the limit.
    # (approx. 5 lines)
    # theta_plus =                                 # Step 1
    # theta_minus =                                # Step 2
    # J_plus =                                    # Step 3
    # J_minus =                                   # Step 4
    # gradapprox =                                # Step 5
    # YOUR CODE STARTS HERE
    theta_plus = theta + epsilon
    theta_minus = theta - epsilon
    J_plus = forward_propagation(x, theta_plus)
    J_minus = forward_propagation(x, theta_minus)
    gradapprox = (J_plus-J_minus)/(2*epsilon)
    
    # YOUR CODE ENDS HERE
    
    # Check if gradapprox is close enough to the output of backward_propagation()
    #(approx. 1 line) DO NOT USE "grad = gradapprox"
    # grad =
    # YOUR CODE STARTS HERE
    grad = backward_propagation(x, theta)
    
    # YOUR CODE ENDS HERE
    
    #(approx. 3 lines)
    # numerator =                                 # Step 1'
    # denominator =                               # Step 2'
    # difference =                                # Step 3'
    # YOUR CODE STARTS HERE
    numerator = np.linalg.norm((grad - gradapprox))
    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)
    difference = numerator/denominator
    
    # YOUR CODE ENDS HERE
    if print_msg:
        if difference > 2e-7:
            print ("\033[93m" + "There is a mistake in the backward propagation! difference = " + str(difference) + "\033[0m")
        else:
            print ("\033[92m" + "Your backward propagation works perfectly fine! difference = " + str(difference) + "\033[0m")
    
    return difference



# GRADED FUNCTION: gradient_check_n

def gradient_check_n(parameters, gradients, X, Y, epsilon=1e-7, print_msg=False):
    """
    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n
    
    Arguments:
    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3"
    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters 
    X -- input datapoint, of shape (input size, number of examples)
    Y -- true "label"
    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)
    
    Returns:
    difference -- difference (2) between the approximated gradient and the backward propagation gradient
    """
    
    # Set-up variables
    parameters_values, _ = dictionary_to_vector(parameters)
    
    grad = gradients_to_vector(gradients)
    num_parameters = parameters_values.shape[0]
    J_plus = np.zeros((num_parameters, 1))
    J_minus = np.zeros((num_parameters, 1))
    gradapprox = np.zeros((num_parameters, 1))
    
    # Compute gradapprox
    for i in range(num_parameters):
        
        # Compute J_plus[i]. Inputs: "parameters_values, epsilon". Output = "J_plus[i]".
        # "_" is used because the function you have outputs two parameters but we only care about the first one
        #(approx. 3 lines)
        # theta_plus =                                        # Step 1
        # theta_plus[i] =                                     # Step 2
        # J_plus[i], _ =                                     # Step 3
        # YOUR CODE STARTS HERE
        theta_plus = np.copy(parameters_values)
        theta_plus[i] = theta_plus[i] + epsilon
        J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(theta_plus))
        
        # YOUR CODE ENDS HERE
        
        # Compute J_minus[i]. Inputs: "parameters_values, epsilon". Output = "J_minus[i]".
        #(approx. 3 lines)
        # theta_minus =                                    # Step 1
        # theta_minus[i] =                                 # Step 2        
        # J_minus[i], _ =                                 # Step 3
        # YOUR CODE STARTS HERE
        theta_minus = np.copy(parameters_values)
        theta_minus[i] = theta_minus[i] - epsilon
        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(theta_minus))
        
        # YOUR CODE ENDS HERE
        
        # Compute gradapprox[i]
        # (approx. 1 line)
        # gradapprox[i] = 
        # YOUR CODE STARTS HERE
        gradapprox[i] = (J_plus[i]-J_minus[i])/(2*epsilon)
        
        # YOUR CODE ENDS HERE
    
    # Compare gradapprox to backward propagation gradients by computing difference.
    # (approx. 3 line)
    # numerator =                                             # Step 1'
    # denominator =                                           # Step 2'
    # difference =                                            # Step 3'
    # YOUR CODE STARTS HERE
    numerator = np.linalg.norm((grad - gradapprox))
    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)
    difference = numerator/denominator
    
    # YOUR CODE ENDS HERE
    if print_msg:
        if difference > 2e-7:
            print ("\033[93m" + "There is a mistake in the backward propagation! difference = " + str(difference) + "\033[0m")
        else:
            print ("\033[92m" + "Your backward propagation works perfectly fine! difference = " + str(difference) + "\033[0m")

    return difference
