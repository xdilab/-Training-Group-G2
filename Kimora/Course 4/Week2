Assignment1:
  # UNQ_C1
  # GRADED FUNCTION: identity_block
  
  def identity_block(X, f, filters, initializer=random_uniform):
      """
      Implementation of the identity block as defined in Figure 4
      
      Arguments:
      X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)
      f -- integer, specifying the shape of the middle CONV's window for the main path
      filters -- python list of integers, defining the number of filters in the CONV layers of the main path
      initializer -- to set up the initial weights of a layer. Equals to random uniform initializer
      
      Returns:
      X -- output of the identity block, tensor of shape (m, n_H, n_W, n_C)
      """
      
      # Retrieve Filters
      F1, F2, F3 = filters
      
      # Save the input value. You'll need this later to add back to the main path. 
      X_shortcut = X
      
      # First component of main path
      X = Conv2D(filters = F1, kernel_size = 1, strides = (1,1), padding = 'valid', kernel_initializer = initializer(seed=0))(X)
      X = BatchNormalization(axis = 3)(X) # Default axis
      X = Activation('relu')(X)
      
      ### START CODE HERE
      ## Second component of main path (≈3 lines)
      ## Set the padding = 'same'
      X = Conv2D(filters = F2, kernel_size = f, strides = (1,1), padding = 'same', kernel_initializer = initializer(seed=0))(X)
      X = BatchNormalization(axis = 3)(X)
      X = Activation('relu')(X)
  
      ## Third component of main path (≈2 lines)
      ## Set the padding = 'valid'
      X = Conv2D(filters = F3, kernel_size = 1, strides = (1,1), padding = 'valid', kernel_initializer = initializer(seed=0))(X)
      X = BatchNormalization(axis = 3)(X) 
      
      ## Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)
      X = Add()([X_shortcut,X])
      X = Activation('relu')(X)
      ### END CODE HERE
  
      return X
  
  
  
  # UNQ_C2
  # GRADED FUNCTION: convolutional_block
  
  def convolutional_block(X, f, filters, s = 2, initializer=glorot_uniform):
      """
      Implementation of the convolutional block as defined in Figure 4
      
      Arguments:
      X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)
      f -- integer, specifying the shape of the middle CONV's window for the main path
      filters -- python list of integers, defining the number of filters in the CONV layers of the main path
      s -- Integer, specifying the stride to be used
      initializer -- to set up the initial weights of a layer. Equals to Glorot uniform initializer, 
                     also called Xavier uniform initializer.
      
      Returns:
      X -- output of the convolutional block, tensor of shape (m, n_H, n_W, n_C)
      """
      
      # Retrieve Filters
      F1, F2, F3 = filters
      
      # Save the input value
      X_shortcut = X
  
  
      ##### MAIN PATH #####
      
      # First component of main path glorot_uniform(seed=0)
      X = Conv2D(filters = F1, kernel_size = 1, strides = (s, s), padding='valid', kernel_initializer = initializer(seed=0))(X)
      X = BatchNormalization(axis = 3)(X)
      X = Activation('relu')(X)
  
      ### START CODE HERE
      
      ## Second component of main path (≈3 lines)
      X = Conv2D(filters = F2, kernel_size = f, strides = (1, 1), padding='same', kernel_initializer = initializer(seed=0))(X)
      X = BatchNormalization(axis = 3)(X)
      X = Activation('relu')(X)
  
      ## Third component of main path (≈2 lines)
      X = Conv2D(filters = F3, kernel_size = 1, strides = (1, 1), padding='valid', kernel_initializer = initializer(seed=0))(X)
      X = BatchNormalization(axis = 3)(X)
      
      ##### SHORTCUT PATH ##### (≈2 lines)
      X_shortcut = Conv2D(filters = F3, kernel_size = 1, strides = (s, s), padding='valid', kernel_initializer = initializer(seed=0))(X_shortcut)
      X_shortcut = BatchNormalization(axis = 3)(X_shortcut)
      
      ### END CODE HERE
  
      # Final step: Add shortcut value to main path (Use this order [X, X_shortcut]), and pass it through a RELU activation
      X = Add()([X, X_shortcut])
      X = Activation('relu')(X)
      
      return X
  
  
  
  # UNQ_C3
  # GRADED FUNCTION: ResNet50
  
  def ResNet50(input_shape = (64, 64, 3), classes = 6, training=False):
      """
      Stage-wise implementation of the architecture of the popular ResNet50:
      CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3
      -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> FLATTEN -> DENSE 
  
      Arguments:
      input_shape -- shape of the images of the dataset
      classes -- integer, number of classes
  
      Returns:
      model -- a Model() instance in Keras
      """
      
      # Define the input as a tensor with shape input_shape
      X_input = Input(input_shape)
  
      
      # Zero-Padding
      X = ZeroPadding2D((3, 3))(X_input)
      
      # Stage 1
      X = Conv2D(64, (7, 7), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)
      X = BatchNormalization(axis = 3)(X)
      X = Activation('relu')(X)
      X = MaxPooling2D((3, 3), strides=(2, 2))(X)
  
      # Stage 2
      X = convolutional_block(X, f = 3, filters = [64, 64, 256], s = 1)
      X = identity_block(X, 3, [64, 64, 256])
      X = identity_block(X, 3, [64, 64, 256])
  
      ### START CODE HERE
      
      # Use the instructions above in order to implement all of the Stages below
      # Make sure you don't miss adding any required parameter
      
      ## Stage 3 (≈4 lines)
      # `convolutional_block` with correct values of `f`, `filters` and `s` for this stage
      X = convolutional_block(X, f = 3, filters = [128, 128, 512], s = 2)
      
      # the 3 `identity_block` with correct values of `f` and `filters` for this stage
      X = identity_block(X, 3, [128, 128, 512])
      X = identity_block(X, 3, [128, 128, 512])
      X = identity_block(X, 3, [128, 128, 512])
  
      # Stage 4 (≈6 lines)
      # add `convolutional_block` with correct values of `f`, `filters` and `s` for this stage
      X = convolutional_block(X, f = 3, filters = [256, 256, 1024], s = 2)
      
      # the 5 `identity_block` with correct values of `f` and `filters` for this stage
      X = identity_block(X, 3, [256, 256, 1024])
      X = identity_block(X, 3, [256, 256, 1024])
      X = identity_block(X, 3, [256, 256, 1024])
      X = identity_block(X, 3, [256, 256, 1024])
      X = identity_block(X, 3, [256, 256, 1024])
  
      # Stage 5 (≈3 lines)
      # add `convolutional_block` with correct values of `f`, `filters` and `s` for this stage
      X = convolutional_block(X, f = 3, filters = [512, 512, 2048], s = 2)
      
      # the 2 `identity_block` with correct values of `f` and `filters` for this stage
      X = identity_block(X, 3, [512, 512, 2048])
      X = identity_block(X, 3, [512, 512, 2048])
  
      # AVGPOOL (≈1 line). Use "X = AveragePooling2D()(X)"
      X = AveragePooling2D(pool_size=(2, 2))(X)
      
      ### END CODE HERE
  
      # output layer
      X = Flatten()(X)
      X = Dense(classes, activation='softmax', kernel_initializer = glorot_uniform(seed=0))(X)
      
      
      # Create model
      model = Model(inputs = X_input, outputs = X)
  
      return model



Assignment2:
  # UNQ_C1
  # GRADED FUNCTION: data_augmenter
  def data_augmenter():
      '''
      Create a Sequential model composed of 2 layers
      Returns:
          tf.keras.Sequential
      '''
      ### START CODE HERE
      data_augmentation = tf.keras.Sequential()
      data_augmentation.add(tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'))
      data_augmentation.add(tf.keras.layers.experimental.preprocessing.RandomRotation(.2))
      ### END CODE HERE
      
      return data_augmentation
  
  
  
  # UNQ_C2
  # GRADED FUNCTION
  def alpaca_model(image_shape=IMG_SIZE, data_augmentation=data_augmenter()):
      ''' Define a tf.keras model for binary classification out of the MobileNetV2 model
      Arguments:
          image_shape -- Image width and height
          data_augmentation -- data augmentation function
      Returns:
      Returns:
          tf.keras.model
      '''
      
      
      input_shape = image_shape + (3,)
      
      ### START CODE HERE
      
      base_model_path="imagenet_base_model/without_top_mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_160_no_top.h5"
      
      base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape,
                                                     include_top=False, # <== Important!!!!
                                                     weights=base_model_path)
      
      # freeze the base model by making it non trainable
      base_model.trainable = False
  
      # create the input layer (Same as the imageNetv2 input size)
      inputs = tf.keras.Input(shape=input_shape) 
      
      # apply data augmentation to the inputs
      x = data_augmentation(inputs)
      
      # data preprocessing using the same weights the model was trained on
      x = tf.keras.applications.mobilenet_v2.preprocess_input(x)
      
      # set training to False to avoid keeping track of statistics in the batch norm layer
      x = base_model(x, training=False) 
      
      # add the new Binary classification layers
      # use global avg pooling to summarize the info in each channel
      x = tf.keras.layers.GlobalAveragePooling2D()(x)
      # include dropout with probability of 0.2 to avoid overfitting
      x = tf.keras.layers.Dropout(0.2)(x)
          
      # use a prediction layer with one neuron (as a binary classifier only needs one)
      outputs = tf.keras.layers.Dense(1, activation='linear')(x)
      
      ### END CODE HERE
      
      model = tf.keras.Model(inputs, outputs)
      
      return model
  
  
  
  # UNQ_C3
  base_model = model2.layers[4]
  base_model.trainable = True
  # Let's take a look to see how many layers are in the base model
  print("Number of layers in the base model: ", len(base_model.layers))
  
  # Fine-tune from this layer onwards
  fine_tune_at = 120
  
  ### START CODE HERE
  
  # Freeze all the layers before the `fine_tune_at` layer
  for layer in base_model.layers[:fine_tune_at]:
      layer.trainable = False
      
  # Define a BinaryCrossentropy loss function. Use from_logits=True
  loss_function=tf.python.keras.losses.BinaryCrossentropy(from_logits=True)
  # Define an Adam optimizer with a learning rate of 0.1 * base_learning_rate
  optimizer = tf.keras.optimizers.Adam(learning_rate=0.1*base_learning_rate)
  # Use accuracy as evaluation metric
  metrics = [tf.keras.metrics.Accuracy(name='accuracy')]
  
  ### END CODE HERE
  
  model2.compile(loss=loss_function,
                optimizer = optimizer,
                metrics=metrics)
