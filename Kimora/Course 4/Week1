Assignment1:
  # GRADED FUNCTION: zero_pad
  
  def zero_pad(X, pad):
      """
      Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, 
      as illustrated in Figure 1.
      
      Argument:
      X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images
      pad -- integer, amount of padding around each image on vertical and horizontal dimensions
      
      Returns:
      X_pad -- padded image of shape (m, n_H + 2 * pad, n_W + 2 * pad, n_C)
      """
      
      #(≈ 1 line)
      # X_pad = None
      # YOUR CODE STARTS HERE
      X_pad = np.pad(X, ((0,0), (pad,pad), (pad,pad), (0,0)), mode='constant', constant_values = (0,0))
      
      # YOUR CODE ENDS HERE
      
      return X_pad
  
  
  
  # GRADED FUNCTION: conv_single_step
  
  def conv_single_step(a_slice_prev, W, b):
      """
      Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation 
      of the previous layer.
      
      Arguments:
      a_slice_prev -- slice of input data of shape (f, f, n_C_prev)
      W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)
      b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)
      
      Returns:
      Z -- a scalar value, the result of convolving the sliding window (W, b) on a slice x of the input data
      """
  
      #(≈ 3 lines of code)
      # Element-wise product between a_slice_prev and W. Do not add the bias yet.
      # s = None
      # Sum over all entries of the volume s.
      # Z = None
      # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.
      # Z = None
      # YOUR CODE STARTS HERE
      s = a_slice_prev * W
      Z = np.sum(s)
      Z = Z + float(b)
      
      # YOUR CODE ENDS HERE
  
      return Z
  
  
  
  # GRADED FUNCTION: conv_forward
  
  def conv_forward(A_prev, W, b, hparameters):
      """
      Implements the forward propagation for a convolution function
      
      Arguments:
      A_prev -- output activations of the previous layer, 
          numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)
      W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)
      b -- Biases, numpy array of shape (1, 1, 1, n_C)
      hparameters -- python dictionary containing "stride" and "pad"
          
      Returns:
      Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)
      cache -- cache of values needed for the conv_backward() function
      """
      
      # Retrieve dimensions from A_prev's shape (≈1 line)  
      # (m, n_H_prev, n_W_prev, n_C_prev) = None
      
      # Retrieve dimensions from W's shape (≈1 line)
      # (f, f, n_C_prev, n_C) = None
      
      # Retrieve information from "hparameters" (≈2 lines)
      # stride = None
      # pad = None
      
      # Compute the dimensions of the CONV output volume using the formula given above. 
      # Hint: use int() to apply the 'floor' operation. (≈2 lines)
      # n_H = None
      # n_W = None
      
      # Initialize the output volume Z with zeros. (≈1 line)
      # Z = None
      
      # Create A_prev_pad by padding A_prev
      # A_prev_pad = None
      
      # for i in range(None):               # loop over the batch of training examples
          # a_prev_pad = None               # Select ith training example's padded activation
          # for h in range(None):           # loop over vertical axis of the output volume
              # Find the vertical start and end of the current "slice" (≈2 lines)
              # vert_start = None
              # vert_end = None
              
              # for w in range(None):       # loop over horizontal axis of the output volume
                  # Find the horizontal start and end of the current "slice" (≈2 lines)
                  # horiz_start = None
                  # horiz_end = None
                  
                  # for c in range(None):   # loop over channels (= #filters) of the output volume
                                          
                      # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)
                      # a_slice_prev = None
                      
                      # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈3 line)
                      # weights = None
                      # biases = None
                      # Z[i, h, w, c] = None
      # YOUR CODE STARTS HERE
      m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape
      f, f, n_C_prev, n_C = W.shape
      stride = hparameters["stride"]
      pad = hparameters["pad"]
      n_H = int((n_H_prev - f + 2 * pad)/stride)+1
      n_W = int((n_W_prev - f + 2 * pad)/stride)+1
      Z = np.zeros((m, n_H, n_W, n_C))
      A_prev_pad = np.pad(A_prev, ((0,), (pad,), (pad,), (0,)), mode='constant', constant_values = 0)
      for i in range(m):
          a_prev_pad = A_prev_pad[i]
          for h in range(n_H):
              vert_start = h * stride
              vert_end = vert_start + f
              for w in range(n_W):
                  horiz_start = w * stride
                  horiz_end = horiz_start + f
                  for c in range(n_C):
                      a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]
                      weights = W[:, :, :, c]
                      biases = b[0, 0, 0, c]
                      Z[i, h, w, c] = np.sum(a_slice_prev * weights) + biases
      
      # YOUR CODE ENDS HERE
      
      # Save information in "cache" for the backprop
      cache = (A_prev, W, b, hparameters)
      
      return Z, cache
  
  
  
  # GRADED FUNCTION: pool_forward
  
  def pool_forward(A_prev, hparameters, mode = "max"):
      """
      Implements the forward pass of the pooling layer
      
      Arguments:
      A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)
      hparameters -- python dictionary containing "f" and "stride"
      mode -- the pooling mode you would like to use, defined as a string ("max" or "average")
      
      Returns:
      A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)
      cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters 
      """
      
      # Retrieve dimensions from the input shape
      (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape
      
      # Retrieve hyperparameters from "hparameters"
      f = hparameters["f"]
      stride = hparameters["stride"]
      
      # Define the dimensions of the output
      n_H = int(1 + (n_H_prev - f) / stride)
      n_W = int(1 + (n_W_prev - f) / stride)
      n_C = n_C_prev
      
      # Initialize output matrix A
      A = np.zeros((m, n_H, n_W, n_C))              
      
      # for i in range(None):                         # loop over the training examples
          # for h in range(None):                     # loop on the vertical axis of the output volume
              # Find the vertical start and end of the current "slice" (≈2 lines)
              # vert_start = None
              # vert_end = None
              
              # for w in range(None):                 # loop on the horizontal axis of the output volume
                  # Find the vertical start and end of the current "slice" (≈2 lines)
                  # horiz_start = None
                  # horiz_end = None
                  
                  # for c in range (None):            # loop over the channels of the output volume
                      
                      # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)
                      # a_prev_slice = None
                      
                      # Compute the pooling operation on the slice. 
                      # Use an if statement to differentiate the modes. 
                      # Use np.max and np.mean.
                      # if mode == "max":
                          # A[i, h, w, c] = None
                      # elif mode == "average":
                          # A[i, h, w, c] = None
      
      # YOUR CODE STARTS HERE
      for i in range(m):
          for h in range(n_H):
              vert_start = h * stride
              vert_end = vert_start + f
              for w in range(n_W):
                  horiz_start = w * stride
                  horiz_end = horiz_start + f
                  for c in range(n_C):
                      a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]
                      if mode == "max":
                          A[i, h, w, c] = np.max(a_prev_slice)
                      elif mode == "average":
                          A[i, h, w, c] = np.average(a_prev_slice)
      
      # YOUR CODE ENDS HERE
      
      # Store the input and hparameters in "cache" for pool_backward()
      cache = (A_prev, hparameters)
      
      # Making sure your output shape is correct
      #assert(A.shape == (m, n_H, n_W, n_C))
      
      return A, cache



Assignment2:
  # GRADED FUNCTION: happyModel
  
  def happyModel():
      """
      Implements the forward propagation for the binary classification model:
      ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> FLATTEN -> DENSE
      
      Note that for simplicity and grading purposes, you'll hard-code all the values
      such as the stride and kernel (filter) sizes. 
      Normally, functions should take these values as function parameters.
      
      Arguments:
      None
  
      Returns:
      model -- TF Keras model (object containing the information for the entire training process) 
      """
      model = tf.keras.Sequential([
              ## ZeroPadding2D with padding 3, input shape of 64 x 64 x 3
              
              ## Conv2D with 32 7x7 filters and stride of 1
              
              ## BatchNormalization for axis 3
              
              ## ReLU
              
              ## Max Pooling 2D with default parameters
              
              ## Flatten layer
              
              ## Dense layer with 1 unit for output & 'sigmoid' activation
              
              # YOUR CODE STARTS HERE
          tfl.ZeroPadding2D(padding=3, input_shape=(64, 64, 3)),
          tfl.Conv2D(filters=32, kernel_size=(7, 7), strides=(1, 1)),
          tfl.BatchNormalization(axis=3),
          tfl.ReLU(),
          tfl.MaxPooling2D(),
          tfl.Flatten(),
          tfl.Dense(1, activation='sigmoid')
              
              # YOUR CODE ENDS HERE
          ])
      
      return model
  
  
  
  # GRADED FUNCTION: convolutional_model
  
  def convolutional_model(input_shape):
      """
      Implements the forward propagation for the model:
      CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> DENSE
      
      Note that for simplicity and grading purposes, you'll hard-code some values
      such as the stride and kernel (filter) sizes. 
      Normally, functions should take these values as function parameters.
      
      Arguments:
      input_img -- input dataset, of shape (input_shape)
  
      Returns:
      model -- TF Keras model (object containing the information for the entire training process) 
      """
  
      input_img = tf.keras.Input(shape=input_shape)
      ## CONV2D: 8 filters 4x4, stride of 1, padding 'SAME'
      # Z1 = None
      ## RELU
      # A1 = None
      ## MAXPOOL: window 8x8, stride 8, padding 'SAME'
      # P1 = None
      ## CONV2D: 16 filters 2x2, stride 1, padding 'SAME'
      # Z2 = None
      ## RELU
      # A2 = None
      ## MAXPOOL: window 4x4, stride 4, padding 'SAME'
      # P2 = None
      ## FLATTEN
      # F = None
      ## Dense layer
      ## 6 neurons in output layer. Hint: one of the arguments should be "activation='softmax'" 
      # outputs = None
      # YOUR CODE STARTS HERE
      Z1 = tf.keras.layers.Conv2D(filters=8, strides=1, kernel_size=4 , padding='same')(input_img)
      A1 = tf.keras.layers.ReLU()(Z1)
      P1 = tf.keras.layers.MaxPool2D(pool_size=8, strides=8, padding='same')(A1)
      Z2 = tf.keras.layers.Conv2D(filters=16, strides=1, kernel_size=2 , padding='same')(P1)
      A2 = tf.keras.layers.ReLU()(Z2)
      P2 = tf.keras.layers.MaxPool2D(pool_size=4, strides=4, padding='same')(A2)
      F = tf.keras.layers.Flatten()(P2)
      outputs = tf.keras.layers.Dense(units=6, activation='softmax')(F)
      
      
      # YOUR CODE ENDS HERE
      model = tf.keras.Model(inputs=input_img, outputs=outputs)
      return model
